# 阶段1向量存储方案技术对比：SQLite vs 向量数据库

## 🎯 问题分析

在**阶段1: OpenAI API + Redis缓存**中，核心问题是选择合适的向量存储方案：
- 是否需要专业向量数据库？
- SQLite + 向量扩展是否足够？
- 内存向量缓存的可行性？

## 📊 技术方案对比

### 方案A：SQLite + sqlite-vss（推荐）

#### 技术实现
```python
import sqlite3
import sqlite_vss
import numpy as np
from typing import List, Tuple

class SQLiteVectorMemory:
    def __init__(self, db_path: str):
        self.db_path = db_path
        self.conn = sqlite3.connect(db_path, check_same_thread=False)
        self._setup_vector_tables()
        
    def _setup_vector_tables(self):
        """初始化SQLite向量搜索"""
        # 加载sqlite-vss扩展
        self.conn.enable_load_extension(True)
        sqlite_vss.load(self.conn)
        
        # 创建向量表
        self.conn.execute("""
            CREATE VIRTUAL TABLE IF NOT EXISTS memory_vectors USING vss0(
                embedding(1536),  -- OpenAI ada-002维度
                +memory_id INTEGER,
                +content TEXT,
                +memory_type TEXT,
                +created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)
        
        # 创建常规内容表（关联存储）
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS memories (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                content TEXT NOT NULL,
                memory_type TEXT DEFAULT 'general',
                metadata JSON,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)
        self.conn.commit()
    
    async def add_memory(self, content: str, embedding: List[float], 
                        memory_type: str = "general", metadata: dict = None) -> int:
        """添加记忆和向量"""
        # 1. 插入常规内容
        cursor = self.conn.execute("""
            INSERT INTO memories (content, memory_type, metadata)
            VALUES (?, ?, ?)
        """, (content, memory_type, json.dumps(metadata or {})))
        
        memory_id = cursor.lastrowid
        
        # 2. 插入向量索引
        self.conn.execute("""
            INSERT INTO memory_vectors (rowid, embedding, memory_id, content, memory_type)
            VALUES (?, ?, ?, ?, ?)
        """, (memory_id, embedding, memory_id, content, memory_type))
        
        self.conn.commit()
        return memory_id
    
    def search_similar_memories(self, query_embedding: List[float], 
                              limit: int = 5, threshold: float = 0.8) -> List[Tuple]:
        """向量相似度搜索"""
        cursor = self.conn.execute("""
            SELECT m.id, m.content, m.memory_type, m.metadata, 
                   v.distance
            FROM memory_vectors v
            JOIN memories m ON v.memory_id = m.id
            WHERE v.embedding MATCH ? AND v.distance < ?
            ORDER BY v.distance
            LIMIT ?
        """, (query_embedding, 1.0 - threshold, limit))
        
        return cursor.fetchall()
```

**优势：**
- ✅ **轻量级**：无需额外服务，基于SQLite
- ✅ **持久化**：数据永久存储，重启不丢失
- ✅ **ACID支持**：事务一致性保障
- ✅ **集成简单**：与现有SQLite架构无缝结合
- ✅ **向量搜索**：支持高效相似度搜索

**劣势：**
- ❌ **性能限制**：大规模向量搜索性能不如专业向量DB
- ❌ **扩展性**：单机存储，无法分布式扩展

### 方案B：内存向量缓存 + SQLite持久化

#### 技术实现
```python
import faiss
import numpy as np
import pickle
from typing import List, Dict, Tuple

class MemoryVectorCache:
    def __init__(self, db_path: str, dimension: int = 1536):
        self.db_path = db_path
        self.dimension = dimension
        
        # FAISS内存索引
        self.index = faiss.IndexFlatIP(dimension)  # 内积相似度
        self.memory_map = {}  # ID -> 内容映射
        self.next_id = 0
        
        # SQLite持久化连接
        self.conn = sqlite3.connect(db_path)
        self._setup_persistence_tables()
        self._load_from_disk()
    
    def _setup_persistence_tables(self):
        """设置持久化表结构"""
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS vector_cache (
                memory_id INTEGER PRIMARY KEY,
                content TEXT NOT NULL,
                embedding BLOB NOT NULL,
                memory_type TEXT DEFAULT 'general',
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)
        self.conn.commit()
    
    def _load_from_disk(self):
        """从SQLite加载向量到内存"""
        cursor = self.conn.execute("""
            SELECT memory_id, content, embedding, memory_type 
            FROM vector_cache ORDER BY memory_id
        """)
        
        embeddings = []
        for row in cursor.fetchall():
            memory_id, content, embedding_blob, memory_type = row
            embedding = pickle.loads(embedding_blob)
            
            # 加载到FAISS索引
            embeddings.append(embedding)
            self.memory_map[memory_id] = {
                'content': content,
                'memory_type': memory_type
            }
            self.next_id = max(self.next_id, memory_id + 1)
        
        if embeddings:
            embeddings_array = np.array(embeddings, dtype=np.float32)
            self.index.add(embeddings_array)
    
    async def add_memory(self, content: str, embedding: List[float], 
                        memory_type: str = "general") -> int:
        """添加记忆到内存缓存和持久化"""
        memory_id = self.next_id
        self.next_id += 1
        
        # 1. 添加到内存索引
        embedding_array = np.array([embedding], dtype=np.float32)
        self.index.add(embedding_array)
        
        # 2. 更新内存映射
        self.memory_map[memory_id] = {
            'content': content,
            'memory_type': memory_type
        }
        
        # 3. 持久化到SQLite
        embedding_blob = pickle.dumps(embedding)
        self.conn.execute("""
            INSERT INTO vector_cache (memory_id, content, embedding, memory_type)
            VALUES (?, ?, ?, ?)
        """, (memory_id, content, embedding_blob, memory_type))
        self.conn.commit()
        
        return memory_id
    
    def search_similar_memories(self, query_embedding: List[float], 
                              k: int = 5) -> List[Tuple]:
        """内存向量搜索"""
        if self.index.ntotal == 0:
            return []
        
        query_array = np.array([query_embedding], dtype=np.float32)
        
        # FAISS搜索
        similarities, indices = self.index.search(query_array, k)
        
        results = []
        for i, (similarity, idx) in enumerate(zip(similarities[0], indices[0])):
            if idx < len(self.memory_map):
                memory_info = self.memory_map[idx]
                results.append((
                    memory_info['content'],
                    float(similarity),
                    memory_info['memory_type']
                ))
        
        return results
```

**优势：**
- ✅ **极高性能**：内存操作，毫秒级搜索
- ✅ **FAISS优化**：专业向量搜索库
- ✅ **灵活性**：可以切换不同索引类型
- ✅ **持久化**：定期保存到SQLite

**劣势：**
- ❌ **内存占用**：所有向量常驻内存
- ❌ **启动时间**：需要加载所有向量到内存
- ❌ **数据一致性**：内存和磁盘可能不同步

### 方案C：专业向量数据库（不推荐阶段1）

```python
# ChromaDB示例
import chromadb

class ChromaDBVectorMemory:
    def __init__(self, collection_name: str = "anp_memories"):
        self.client = chromadb.Client()
        self.collection = self.client.create_collection(
            name=collection_name,
            metadata={"hnsw:space": "cosine"}
        )
    
    async def add_memory(self, content: str, embedding: List[float], 
                        memory_id: str) -> None:
        self.collection.add(
            embeddings=[embedding],
            documents=[content],
            ids=[memory_id]
        )
    
    def search_similar_memories(self, query_embedding: List[float], 
                              n_results: int = 5) -> List:
        results = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=n_results
        )
        return results
```

**优势：**
- ✅ **专业性能**：为向量搜索优化
- ✅ **丰富功能**：支持过滤、聚合等高级功能
- ✅ **可扩展性**：支持分布式部署

**劣势：**
- ❌ **复杂度高**：需要额外服务和依赖
- ❌ **资源占用**：独立进程，占用更多资源
- ❌ **过度设计**：阶段1不需要如此复杂

## 🎯 阶段1推荐方案

### 最优选择：SQLite + sqlite-vss + 内存LRU缓存

```python
from functools import lru_cache
import sqlite_vss

class OptimalVectorMemory:
    def __init__(self, db_path: str, cache_size: int = 1000):
        self.db_path = db_path
        self.cache_size = cache_size
        self.conn = sqlite3.connect(db_path)
        sqlite_vss.load(self.conn)
        self._setup_tables()
    
    @lru_cache(maxsize=1000)
    def _cached_search(self, query_hash: str, query_embedding_str: str) -> List:
        """LRU缓存的搜索结果"""
        query_embedding = eval(query_embedding_str)  # 实际应用中用更安全的方法
        return self._raw_search(query_embedding)
    
    def _raw_search(self, query_embedding: List[float]) -> List:
        """原始向量搜索"""
        cursor = self.conn.execute("""
            SELECT content, distance FROM memory_vectors 
            WHERE embedding MATCH ? 
            ORDER BY distance LIMIT 5
        """, (query_embedding,))
        return cursor.fetchall()
    
    def search_with_cache(self, query_embedding: List[float]) -> List:
        """带缓存的搜索"""
        import hashlib
        query_hash = hashlib.md5(str(query_embedding).encode()).hexdigest()
        return self._cached_search(query_hash, str(query_embedding))
```

## ✅ 最终建议

**阶段1推荐：SQLite + sqlite-vss + 内存缓存**

### 理由：
1. **简单性**：无需额外服务，基于现有SQLite架构
2. **可靠性**：SQLite的ACID特性保证数据一致性
3. **性能平衡**：sqlite-vss提供足够的向量搜索性能
4. **渐进式**：后续可以无缝升级到更复杂方案

### 实施策略：
```python
# 阶段1核心架构
class Stage1VectorMemory:
    def __init__(self):
        self.sqlite_vector = SQLiteVectorMemory("memory.db")
        self.redis_cache = EmbeddingCache(redis_client)  # embedding缓存
        self.result_cache = {}  # 搜索结果LRU缓存
        
    async def add_memory(self, content: str):
        # 1. 生成embedding（带Redis缓存）
        embedding = await self.get_cached_embedding(content)
        # 2. 存储到SQLite向量表
        return await self.sqlite_vector.add_memory(content, embedding)
    
    async def search_memories(self, query: str):
        # 1. 生成query embedding
        query_embedding = await self.get_cached_embedding(query)
        # 2. SQLite向量搜索（带结果缓存）
        return self.sqlite_vector.search_similar_memories(query_embedding)
```

这个方案既保持了架构简单性，又具备了向更复杂方案演进的能力！